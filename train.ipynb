{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "sys.path.append('core')\n",
    "\n",
    "import argparse, configparser\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.optim import Adam as AdamW\n",
    "from torch.optim.adamw import AdamW\n",
    "from core.onecyclelr import OneCycleLR\n",
    "\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from torch.cuda.amp import GradScaler\n",
    "except:\n",
    "    # dummy GradScaler for PyTorch < 1.6\n",
    "    class GradScaler:\n",
    "        def __init__(self, enabled=False):\n",
    "            pass\n",
    "        def scale(self, loss):\n",
    "            return loss\n",
    "        def unscale_(self, optimizer):\n",
    "            pass\n",
    "        def step(self, optimizer):\n",
    "            optimizer.step()\n",
    "        def update(self):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "lr = 0.0002\n",
    "# num_steps = 100000\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "image_size = [160, 240]\n",
    "gpuid = 0\n",
    "iter = 12\n",
    "wdecay = 0.0005\n",
    "epsilon = 1e-8\n",
    "clip = 1\n",
    "dropout = 0\n",
    "gamma = 0.8\n",
    "add_noise = False\n",
    "\n",
    "torch.set_num_threads(16)\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making directories for saving checkpoints and logs\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.mkdir('checkpoints')\n",
    "if not os.path.exists('runs'):\n",
    "    os.mkdir('runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from glob import glob\n",
    "import os.path as osp\n",
    "\n",
    "from utils import frame_utils\n",
    "from utils.augmentor import FlowAugmentor, SparseFlowAugmentor\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, aug_params=None, sparse=False):\n",
    "        self.augmentor = None\n",
    "        self.sparse = sparse\n",
    "        if aug_params is not None:\n",
    "            if sparse:\n",
    "                self.augmentor = SparseFlowAugmentor(**aug_params)\n",
    "            else:\n",
    "                self.augmentor = FlowAugmentor(**aug_params)\n",
    "\n",
    "        self.is_test = False\n",
    "        self.is_validate = False\n",
    "        self.init_seed = False\n",
    "        self.flow_list = []\n",
    "        self.image_list = []\n",
    "        self.extra_info = []\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print('Index is {}'.format(index))\n",
    "        # sys.stdout.flush()\n",
    "        if self.is_test:\n",
    "            img1 = frame_utils.read_gen(self.image_list[index][0])\n",
    "            img2 = frame_utils.read_gen(self.image_list[index][1])\n",
    "            img1 = np.array(img1).astype(np.uint8)[..., :3]\n",
    "            img2 = np.array(img2).astype(np.uint8)[..., :3]\n",
    "            img1 = torch.from_numpy(img1).permute(2, 0, 1).float()\n",
    "            img2 = torch.from_numpy(img2).permute(2, 0, 1).float()\n",
    "            return img1, img2, self.extra_info[index]\n",
    "\n",
    "        # if not self.init_seed:\n",
    "        #     worker_info = torch.utils.data.get_worker_info()\n",
    "        #     if worker_info is not None:\n",
    "        #         torch.manual_seed(worker_info.id)\n",
    "        #         np.random.seed(worker_info.id)\n",
    "        #         random.seed(worker_info.id)\n",
    "        #         self.init_seed = True\n",
    "\n",
    "        index = index % len(self.image_list)\n",
    "        valid = None\n",
    "        h,w = 600,800\n",
    "        if self.sparse:\n",
    "            flow, valid = frame_utils.readFlowKITTI(self.flow_list[index])\n",
    "        else:\n",
    "            flow = frame_utils.read_gen(self.flow_list[index],h = h,w = w)\n",
    "\n",
    "        img1 = frame_utils.read_gen(self.image_list[index][0])\n",
    "        img2 = frame_utils.read_gen(self.image_list[index][1])\n",
    "\n",
    "        flow = np.array(flow).astype(np.float32)\n",
    "        img1 = np.array(img1).astype(np.uint8)\n",
    "        img2 = np.array(img2).astype(np.uint8)\n",
    "\n",
    "        # grayscale images\n",
    "        if len(img1.shape) == 2:\n",
    "            img1 = np.tile(img1[...,None], (1, 1, 3))\n",
    "            img2 = np.tile(img2[...,None], (1, 1, 3))\n",
    "        else:\n",
    "            img1 = img1[..., :3]\n",
    "            img2 = img2[..., :3]\n",
    "\n",
    "        if self.augmentor is not None:\n",
    "            if self.sparse:\n",
    "                img1, img2, flow, valid = self.augmentor(img1, img2, flow, valid)\n",
    "            else:\n",
    "                img1, img2, flow = self.augmentor(img1, img2, flow)\n",
    "\n",
    "        img1 = torch.from_numpy(img1).permute(2, 0, 1).float()\n",
    "        img2 = torch.from_numpy(img2).permute(2, 0, 1).float()\n",
    "        flow = torch.from_numpy(flow).permute(2, 0, 1).float()\n",
    "\n",
    "        if valid is not None:\n",
    "            valid = torch.from_numpy(valid)\n",
    "        else:\n",
    "            valid = (flow[0].abs() < 1000) & (flow[1].abs() < 1000)\n",
    "\n",
    "        if self.is_validate:\n",
    "            return img1, img2, flow, valid.float(), self.extra_info[index]\n",
    "        else:\n",
    "            return img1, img2, flow, valid.float()\n",
    "\n",
    "    def getDataWithPath(self, index):\n",
    "        img1, img2, flow, valid = self.__getitem__(index)\n",
    "\n",
    "        imgPath_1 = self.image_list[index][0]\n",
    "        imgPath_2 = self.image_list[index][1]\n",
    "\n",
    "        return img1, img2, flow, valid, imgPath_1, imgPath_2\n",
    "\n",
    "    def __rmul__(self, v):\n",
    "        self.flow_list = v * self.flow_list\n",
    "        self.image_list = v * self.image_list\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Carla_Dataset(FlowDataset):\n",
    "    def __init__(self, aug_params=None, split='training', root='/home/sushlok/new_approach/datasets/carla', \n",
    "                 seq= [\n",
    "                    \"SoftRainNight\",\n",
    "                    \"ClearNoon\",\n",
    "                    \"CloudyNoon\"\n",
    "                ], \n",
    "            setup_type = [ 'camera_0', 'camera_-1','camera_1'], is_validate=False):\n",
    "        super(Carla_Dataset, self).__init__(aug_params, sparse=False)\n",
    "        if split == 'testing':\n",
    "            self.is_test = True\n",
    "\n",
    "        self.is_validate = is_validate\n",
    "        image_dirs = []\n",
    "        # datasets/vkitti/vkitti_1.3.1_rgb\n",
    "        # print(seq, setup_type)\n",
    "        for s in seq:\n",
    "            for t in setup_type:\n",
    "                # print(sorted(glob(osp.join(root, '%s' %(s) ,'rgb_%s/*.png' % (t)))))\n",
    "                image_dirs += sorted(glob(osp.join(root, '%s' %(s) ,'rgb_%s/*.png' % (t))))\n",
    "        # print(image_dirs)\n",
    "        for i in range(len(image_dirs)-1):\n",
    "            img1 = image_dirs[i]\n",
    "            img2 = image_dirs[i+1]  \n",
    "            self.image_list += [ [img2, img1] ]\n",
    "            self.extra_info += [ [img2.split('/')[-1]] ]\n",
    "            \n",
    "        if split == 'training':\n",
    "            for s in seq:\n",
    "                for t in setup_type:\n",
    "                    self.flow_list += sorted(glob(osp.join(root, '%s' %(s) ,'flow_%s/flow_npz/*.npz' % (t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_params = {'crop_size': image_size, 'min_scale': -0.2, 'max_scale': 0.4, 'do_flip': False}\n",
    "train_dataset = Carla_Dataset(aug_params, split='training', seq= [\"MidRainSunset\"], setup_type = ['camera_1']) # , 'camera_1','camera_2','camera_3', 'camera_4'\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data in dataloader using tensorboard\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter()\n",
    "# for i, (img1, img2, flow, valid) in enumerate(train_loader):\n",
    "#     # print(img1.shape, img2.shape, flow.shape, valid.shape)\n",
    "#     writer.add_images('img1', img1, i)\n",
    "#     writer.add_images('img2', img2, i)\n",
    "#     # writer.add_images('flow', flow, i)\n",
    "#     # writer.add_images('valid', valid, i)\n",
    "# writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from core.update import BasicUpdateBlock\n",
    "from core.extractor import BasicEncoder, BasicConvEncoder, Non_uniform_Encoder\n",
    "from core.corr import CorrBlock, AlternateCorrBlock\n",
    "from utils.utils import bilinear_sampler, coords_grid, upflow8\n",
    "from core.swin_transformer import POLAUpdate, MixAxialPOLAUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast = torch.cuda.amp.autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMFlowNetModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.hidden_dim = hdim = 128\n",
    "        self.context_dim = cdim = 128\n",
    "        args.corr_levels = 4\n",
    "        args.corr_radius = 4\n",
    "        args.dropout = 0.0\n",
    "        args.use_mix_attn = False\n",
    "        args.mixed_precision = True\n",
    "        if not hasattr(self.args, 'dropout'):\n",
    "            self.args.dropout = 0\n",
    "\n",
    "        if not hasattr(self.args, 'alternate_corr'):\n",
    "            self.args.alternate_corr = False\n",
    "\n",
    "        # feature network, context network, and update block\n",
    "        if self.args.use_mix_attn:\n",
    "            self.fnet = nn.Sequential(\n",
    "                            # BasicConvEncoder(output_dim=256, norm_fn='instance', dropout=args.dropout),\n",
    "                            Non_uniform_Encoder(output_dim=256, norm_fn='instance', dropout=args.dropout),\n",
    "                            MixAxialPOLAUpdate(embed_dim=256, depth=6, num_head=8, window_size=7)\n",
    "                        )\n",
    "        else:\n",
    "            self.fnet = nn.Sequential(\n",
    "                Non_uniform_Encoder(output_dim=256, norm_fn='instance', dropout=args.dropout),\n",
    "                # BasicConvEncoder(output_dim=256, norm_fn='instance', dropout=args.dropout),\n",
    "                POLAUpdate(embed_dim=256, depth=6, num_head=8, window_size=7, neig_win_num=1)\n",
    "            )\n",
    "\n",
    "        self.cnet = BasicEncoder(output_dim=hdim+cdim, norm_fn='batch', dropout=args.dropout)\n",
    "        self.update_block = BasicUpdateBlock(self.args, hidden_dim=hdim, input_dim=cdim)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def initialize_flow(self, img):\n",
    "        \"\"\" Flow is represented as difference between two coordinate grids flow = coords1 - coords0\"\"\"\n",
    "        N, C, H, W = img.shape\n",
    "        coords0 = coords_grid(N, H//8, W//8).to(img.device)\n",
    "        coords1 = coords_grid(N, H//8, W//8).to(img.device)\n",
    "\n",
    "        # optical flow computed as difference: flow = coords1 - coords0\n",
    "        return coords0, coords1\n",
    "\n",
    "    def upsample_flow(self, flow, mask):\n",
    "        \"\"\" Upsample flow field [H/8, W/8, 2] -> [H, W, 2] using convex combination \"\"\"\n",
    "        N, _, H, W = flow.shape\n",
    "        mask = mask.view(N, 1, 9, 8, 8, H, W)\n",
    "        mask = torch.softmax(mask, dim=2)\n",
    "\n",
    "        up_flow = F.unfold(8 * flow, [3,3], padding=1)\n",
    "        up_flow = up_flow.view(N, 2, 9, 1, 1, H, W)\n",
    "\n",
    "        up_flow = torch.sum(mask * up_flow, dim=2)\n",
    "        up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)\n",
    "        return up_flow.reshape(N, 2, 8*H, 8*W)\n",
    "\n",
    "    def forward(self, image1, image2, iters=12, flow_init=None, upsample=True, test_mode=False):\n",
    "        \"\"\" Estimate optical flow between pair of frames \"\"\"\n",
    "\n",
    "        image1 = 2 * (image1 / 255.0) - 1.0\n",
    "        image2 = 2 * (image2 / 255.0) - 1.0\n",
    "\n",
    "        image1 = image1.contiguous()\n",
    "        image2 = image2.contiguous()\n",
    "\n",
    "        hdim = self.hidden_dim\n",
    "        cdim = self.context_dim\n",
    "\n",
    "        # run the feature network\n",
    "        with autocast(enabled=self.args.mixed_precision):\n",
    "            fmaps, cache = self.fnet([image1, image2])\n",
    "        fmap1,fmap2 = fmaps\n",
    "        fmap1 = fmap1.float()\n",
    "        fmap2 = fmap2.float()\n",
    "\n",
    "        # # Self-attention update\n",
    "        # fmap1 = self.transEncoder(fmap1)\n",
    "        # fmap2 = self.transEncoder(fmap2)\n",
    "        # print(\"feature_volume:\")\n",
    "        # # print(fmap1, fmap2)\n",
    "        # print(fmap1.shape)\n",
    "        # print(\"------------------------------------------------\")\n",
    "        if self.args.alternate_corr:\n",
    "            corr_fn = AlternateCorrBlock(fmap1, fmap2, radius=self.args.corr_radius)\n",
    "        else:\n",
    "            corr_fn = CorrBlock(fmap1, fmap2, radius=self.args.corr_radius)\n",
    "\n",
    "        # print(\"corr_fn:\")\n",
    "        # # print(corr_fn.corrMap)\n",
    "        # print(\"------------------------------------------------\")\n",
    "        # print(corr_fn.corrMap.shape)\n",
    "        # run the context network\n",
    "        with autocast(enabled=self.args.mixed_precision):\n",
    "            cnet = self.cnet(image1)\n",
    "            net, inp = torch.split(cnet, [hdim, cdim], dim=1)\n",
    "            net = torch.tanh(net)\n",
    "            inp = torch.relu(inp)\n",
    "\n",
    "        coords0, coords1 = self.initialize_flow(image1)\n",
    "\n",
    "        # Correlation as initialization\n",
    "        N, fC, fH, fW = fmap1.shape\n",
    "        corrMap = corr_fn.corrMap\n",
    "\n",
    "        #_, coords_index = torch.max(corrMap, dim=-1) # no gradient here\n",
    "        softCorrMap = F.softmax(corrMap, dim=2) * F.softmax(corrMap, dim=1) # (N, fH*fW, fH*fW)\n",
    "\n",
    "        # print(\"softCorrMap:\")\n",
    "        # # print(softCorrMap)\n",
    "        # print(\"------------------------------------------------\")\n",
    "        # print(softCorrMap.shape)\n",
    "        if flow_init is not None:\n",
    "            coords1 = coords1 + flow_init\n",
    "        else:\n",
    "            # print('matching as init')\n",
    "            # mutual match selection\n",
    "            match12, match_idx12 = softCorrMap.max(dim=2) # (N, fH*fW)\n",
    "            match21, match_idx21 = softCorrMap.max(dim=1)\n",
    "\n",
    "            for b_idx in range(N):\n",
    "                match21_b = match21[b_idx,:]\n",
    "                match_idx12_b = match_idx12[b_idx,:]\n",
    "                match21[b_idx,:] = match21_b[match_idx12_b]\n",
    "\n",
    "            matched = (match12 - match21) == 0  # (N, fH*fW)\n",
    "            coords_index = torch.arange(fH*fW).unsqueeze(0).repeat(N,1).to(softCorrMap.device)\n",
    "            coords_index[matched] = match_idx12[matched]\n",
    "\n",
    "            # matched coords\n",
    "            coords_index = coords_index.reshape(N, fH, fW)\n",
    "            coords_x = coords_index % fW\n",
    "            coords_y = coords_index // fW\n",
    "\n",
    "            coords_xy = torch.stack([coords_x, coords_y], dim=1).float()\n",
    "            coords1 = coords_xy\n",
    "        # print('coords1:')\n",
    "        # # print(coords1)\n",
    "        # print(coords1.shape)\n",
    "        # print(\"------------------------------------------------\")\n",
    "        \n",
    "        # Iterative update\n",
    "        flow_predictions = []\n",
    "        print(\"iter:\",iter)\n",
    "        for itr in range(iters):\n",
    "            print(itr)\n",
    "            coords1 = coords1.detach()\n",
    "            corr = corr_fn(coords1) # index correlation volume\n",
    "\n",
    "            flow = coords1 - coords0\n",
    "            # print(\"flow:\")\n",
    "            # print(flow)\n",
    "            # print(flow.shape)\n",
    "            # print(\"------------------------------------------------\")\n",
    "            with autocast(enabled=self.args.mixed_precision):\n",
    "                net, up_mask, delta_flow = self.update_block(net, inp, corr, flow)\n",
    "\n",
    "            # F(t+1) = F(t) + \\Delta(t)\n",
    "            \n",
    "            coords1 = coords1 + delta_flow\n",
    "            # print(\"coords1:\")\n",
    "            # # print(coords1)\n",
    "            # print(coords1.shape)\n",
    "            # print(\"------------------------------------------------\")\n",
    "            \n",
    "            # upsample predictions\n",
    "            if up_mask is None:\n",
    "                flow_up = upflow8(coords1 - coords0)\n",
    "            else:\n",
    "                flow_up = self.upsample_flow(coords1 - coords0, up_mask)\n",
    "            print(flow_up.shape)\n",
    "            flow_predictions.append(flow_up)\n",
    "\n",
    "        if test_mode:\n",
    "            return coords1 - coords0, flow_up\n",
    "\n",
    "        return flow_predictions, softCorrMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define args \n",
    "args = argparse.Namespace()\n",
    "model = nn.DataParallel(GMFlowNetModel(args), device_ids=[gpuid])\n",
    "# model = GMFlowNetModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(\"pretrained_models/new_model.pth\"), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model using tensorboard\n",
    "# writer = SummaryWriter(\"runs/test_runs\")\n",
    "# test_image = 255*torch.randn(1, 3, 64, 64)\n",
    "# writer.add_graph(model, (test_image,test_image.transpose(2,3)))\n",
    "# writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wdecay, eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scheduler\n",
    "from core.onecyclelr import OneCycleLR\n",
    "scheduler = OneCycleLR(optimizer, lr, steps_per_epoch=len(train_loader), epochs=epochs,\n",
    "        pct_start=0.05, cycle_momentum=False, anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler(enabled=args.mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.loss import compute_supervision_coarse, compute_coarse_loss, backwarp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FLOW = 400\n",
    "SUM_FREQ = 100\n",
    "VAL_FREQ = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def sequence_loss(train_outputs, image1, image2, flow_gt, valid, gamma=0.8, max_flow=MAX_FLOW, use_matching_loss=False):\n",
    "    \"\"\" Loss function defined over sequence of flow predictions \"\"\"\n",
    "    flow_preds, softCorrMap = train_outputs\n",
    "\n",
    "    # original RAFT loss\n",
    "    n_predictions = len(flow_preds)\n",
    "    flow_loss = 0.0\n",
    "\n",
    "    # exclude invalid pixels and extremely large displacements\n",
    "    mag = torch.sum(flow_gt**2, dim=1).sqrt()\n",
    "    valid = (valid >= 0.5) & (mag < max_flow)\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        i_weight = gamma**(n_predictions - i - 1)\n",
    "        i_loss = (flow_preds[i] - flow_gt).abs()\n",
    "        flow_loss += i_weight * (valid[:, None].float()  * i_loss).mean()\n",
    "\n",
    "    epe = torch.sum((flow_preds[-1] - flow_gt)**2, dim=1).sqrt()\n",
    "    epe = epe.view(-1)[valid.view(-1)]\n",
    "\n",
    "    metrics = {\n",
    "        'epe': epe.mean().item(),\n",
    "        '1px': (epe < 1).float().mean().item(),\n",
    "        '3px': (epe < 3).float().mean().item(),\n",
    "        '5px': (epe < 5).float().mean().item(),\n",
    "    }\n",
    "\n",
    "    if use_matching_loss:\n",
    "        # enable global matching loss. Try to use it in late stages of the trianing\n",
    "        img_2back1 = backwarp(image2, flow_gt)\n",
    "        occlusionMap = (image1 - img_2back1).mean(1, keepdims=True) #(N, H, W)\n",
    "        occlusionMap = torch.abs(occlusionMap) > 20\n",
    "        occlusionMap = occlusionMap.float()\n",
    "\n",
    "        conf_matrix_gt = compute_supervision_coarse(flow_gt, occlusionMap, 8) # 8 from RAFT downsample\n",
    "\n",
    "        matchLossCfg = configparser.ConfigParser()\n",
    "        matchLossCfg.POS_WEIGHT = 1\n",
    "        matchLossCfg.NEG_WEIGHT = 1\n",
    "        matchLossCfg.FOCAL_ALPHA = 0.25\n",
    "        matchLossCfg.FOCAL_GAMMA = 2.0\n",
    "        matchLossCfg.COARSE_TYPE = 'cross_entropy'\n",
    "        match_loss = compute_coarse_loss(softCorrMap, conf_matrix_gt, matchLossCfg)\n",
    "\n",
    "        flow_loss = flow_loss + 0.01*match_loss\n",
    "\n",
    "    return flow_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): GMFlowNetModel(\n",
       "    (fnet): Sequential(\n",
       "      (0): Non_uniform_Encoder(\n",
       "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm3): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (residual_layer1): Diff_ResidualBlock(\n",
       "          (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (residual_layer2): Diff_ResidualBlock(\n",
       "          (conv0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (residual_layer3): Diff_ResidualBlock(\n",
       "          (conv0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (grad_mask1): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (grad_mask2): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (grad_mask3): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (downscale1): Gradient_based_Downsampling(\n",
       "          (softmax): Softmax(dim=2)\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (downscale2): Gradient_based_Downsampling(\n",
       "          (softmax): Softmax(dim=2)\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (downscale3): Gradient_based_Downsampling(\n",
       "          (softmax): Softmax(dim=2)\n",
       "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (1): POLAUpdate(\n",
       "        (blocks): ModuleList(\n",
       "          (0): POLATransBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): NeighborWindowAttention(\n",
       "              (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): POLATransBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): NeighborWindowAttention(\n",
       "              (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): POLATransBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): NeighborWindowAttention(\n",
       "              (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): POLATransBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): NeighborWindowAttention(\n",
       "              (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): POLATransBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): NeighborWindowAttention(\n",
       "              (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): POLATransBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): NeighborWindowAttention(\n",
       "              (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (cnet): BasicEncoder(\n",
       "      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (layer1): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): ResidualBlock(\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (update_block): BasicUpdateBlock(\n",
       "      (encoder): BasicMotionEncoder(\n",
       "        (convc1): Conv2d(324, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (convc2): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (convf1): Conv2d(2, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (convf2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (gru): SepConvGRU(\n",
       "        (convz1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "        (convr1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "        (convq1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "        (convz2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "        (convr2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "        (convq2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "      )\n",
       "      (flow_head): FlowHead(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (mask): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting model to train mode\n",
    "model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  43.149169921875\n",
      "metrics:  {'epe': 15.403193473815918, '1px': 0.2387109398841858, '3px': 0.33048176765441895, '5px': 0.42722657322883606}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  160.16995239257812\n",
      "metrics:  {'epe': 69.9950942993164, '1px': 0.1131705790758133, '3px': 0.2483789175748825, '5px': 0.25240886211395264}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  22.75017738342285\n",
      "metrics:  {'epe': 7.093921184539795, '1px': 0.056204427033662796, '3px': 0.4962630271911621, '5px': 0.5897982120513916}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  10.249296188354492\n",
      "metrics:  {'epe': 2.7934930324554443, '1px': 0.3038671910762787, '3px': 0.6585416793823242, '5px': 0.8293945789337158}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  57.2637825012207\n",
      "metrics:  {'epe': 26.433547973632812, '1px': 0.2539192736148834, '3px': 0.2941536605358124, '5px': 0.3351302146911621}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  89.63540649414062\n",
      "metrics:  {'epe': 33.26653289794922, '1px': 0.013645833358168602, '3px': 0.12323568016290665, '5px': 0.2179947942495346}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  91.94039916992188\n",
      "metrics:  {'epe': 36.34040832519531, '1px': 0.004082031548023224, '3px': 0.07531901448965073, '5px': 0.12757812440395355}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  55.011417388916016\n",
      "metrics:  {'epe': 17.83380889892578, '1px': 0.0025455730501562357, '3px': 0.14902344346046448, '5px': 0.3127018213272095}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  36.87044143676758\n",
      "metrics:  {'epe': 14.1046781539917, '1px': 0.18252605199813843, '3px': 0.4578971564769745, '5px': 0.4925781488418579}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  74.71855163574219\n",
      "metrics:  {'epe': 26.130062103271484, '1px': 0.023971354588866234, '3px': 0.1271093785762787, '5px': 0.21865886449813843}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  116.3432846069336\n",
      "metrics:  {'epe': 38.12355422973633, '1px': 0.01536458358168602, '3px': 0.06918620318174362, '5px': 0.1226171925663948}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  21.60749053955078\n",
      "metrics:  {'epe': 7.545428276062012, '1px': 0.256451815366745, '3px': 0.3394661545753479, '5px': 0.4592578113079071}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  23.831022262573242\n",
      "metrics:  {'epe': 8.28109359741211, '1px': 0.1364973932504654, '3px': 0.48510417342185974, '5px': 0.794700562953949}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  36.05813980102539\n",
      "metrics:  {'epe': 16.53526496887207, '1px': 0.049108073115348816, '3px': 0.2674218714237213, '5px': 0.36727866530418396}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  79.52521514892578\n",
      "metrics:  {'epe': 28.50497817993164, '1px': 0.008111979812383652, '3px': 0.09676432609558105, '5px': 0.2004687488079071}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  53.920928955078125\n",
      "metrics:  {'epe': 20.27758026123047, '1px': 0.00706380233168602, '3px': 0.07182291895151138, '5px': 0.2904296815395355}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  75.1307373046875\n",
      "metrics:  {'epe': 27.594449996948242, '1px': 0.01328125037252903, '3px': 0.09549479186534882, '5px': 0.21643880009651184}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  39.01411437988281\n",
      "metrics:  {'epe': 15.291644096374512, '1px': 0.10131510347127914, '3px': 0.2739843726158142, '5px': 0.3419531285762787}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.707360744476318\n",
      "metrics:  {'epe': 2.5159082412719727, '1px': 0.43460288643836975, '3px': 0.6866406202316284, '5px': 0.8306249976158142}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  39.58662414550781\n",
      "metrics:  {'epe': 15.024261474609375, '1px': 0.02393229305744171, '3px': 0.18075521290302277, '5px': 0.26522788405418396}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  24.325796127319336\n",
      "metrics:  {'epe': 7.603030681610107, '1px': 0.028567709028720856, '3px': 0.2698437571525574, '5px': 0.6137109398841858}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  43.26155090332031\n",
      "metrics:  {'epe': 14.215195655822754, '1px': 0.013717448338866234, '3px': 0.3269466161727905, '5px': 0.3664257824420929}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  18.25267791748047\n",
      "metrics:  {'epe': 6.124075889587402, '1px': 0.02197265625, '3px': 0.21161459386348724, '5px': 0.5034961104393005}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  23.58089828491211\n",
      "metrics:  {'epe': 9.833735466003418, '1px': 0.08980468660593033, '3px': 0.42223307490348816, '5px': 0.5494987368583679}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  34.037445068359375\n",
      "metrics:  {'epe': 11.179627418518066, '1px': 0.39111328125, '3px': 0.5049088597297668, '5px': 0.5322656631469727}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  27.26375961303711\n",
      "metrics:  {'epe': 8.705408096313477, '1px': 0.4362435042858124, '3px': 0.6324934959411621, '5px': 0.6483594179153442}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  8.050408363342285\n",
      "metrics:  {'epe': 2.4355127811431885, '1px': 0.34275391697883606, '3px': 0.7397135496139526, '5px': 0.8732617497444153}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  27.20904541015625\n",
      "metrics:  {'epe': 8.58908748626709, '1px': 0.3395768404006958, '3px': 0.5659961104393005, '5px': 0.6112369894981384}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  14.224262237548828\n",
      "metrics:  {'epe': 4.509566307067871, '1px': 0.6371744871139526, '3px': 0.7748112082481384, '5px': 0.8137825727462769}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  24.382131576538086\n",
      "metrics:  {'epe': 8.537345886230469, '1px': 0.5540950894355774, '3px': 0.6536263227462769, '5px': 0.7284179925918579}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.055094242095947\n",
      "metrics:  {'epe': 1.159280776977539, '1px': 0.49442058801651, '3px': 0.9737955927848816, '5px': 0.9996809959411621}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  23.54405975341797\n",
      "metrics:  {'epe': 7.935920715332031, '1px': 0.35404297709465027, '3px': 0.7392513155937195, '5px': 0.7936719059944153}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  47.21713638305664\n",
      "metrics:  {'epe': 15.765316009521484, '1px': 0.059941407293081284, '3px': 0.3373437523841858, '5px': 0.453509122133255}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  11.954773902893066\n",
      "metrics:  {'epe': 3.8779895305633545, '1px': 0.28055340051651, '3px': 0.5199934840202332, '5px': 0.6923112273216248}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.362710952758789\n",
      "metrics:  {'epe': 1.961989402770996, '1px': 0.2973242402076721, '3px': 0.8966927528381348, '5px': 0.93603515625}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  25.48957633972168\n",
      "metrics:  {'epe': 8.644763946533203, '1px': 0.3051692843437195, '3px': 0.5418489575386047, '5px': 0.6338151097297668}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  35.92061233520508\n",
      "metrics:  {'epe': 14.08078670501709, '1px': 0.030039062723517418, '3px': 0.19307291507720947, '5px': 0.4771549701690674}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  12.14195442199707\n",
      "metrics:  {'epe': 4.250723361968994, '1px': 0.011777344159781933, '3px': 0.2722851634025574, '5px': 0.8446484804153442}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.082643508911133\n",
      "metrics:  {'epe': 2.258767604827881, '1px': 0.5470442771911621, '3px': 0.7764974236488342, '5px': 0.848717451095581}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.9723243713378906\n",
      "metrics:  {'epe': 1.3423914909362793, '1px': 0.5497135519981384, '3px': 0.9280599355697632, '5px': 0.9912695288658142}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  31.448444366455078\n",
      "metrics:  {'epe': 9.239526748657227, '1px': 0.09913411736488342, '3px': 0.27763673663139343, '5px': 0.3554622530937195}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.4693379402160645\n",
      "metrics:  {'epe': 2.1605582237243652, '1px': 0.3349739611148834, '3px': 0.6749544143676758, '5px': 0.9168815612792969}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  15.590864181518555\n",
      "metrics:  {'epe': 5.487434387207031, '1px': 0.21222656965255737, '3px': 0.7066797018051147, '5px': 0.8454166650772095}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.2155888080596924\n",
      "metrics:  {'epe': 0.9051325917243958, '1px': 0.7145182490348816, '3px': 0.9438411593437195, '5px': 0.9713606834411621}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.874658107757568\n",
      "metrics:  {'epe': 2.271013021469116, '1px': 0.3382031321525574, '3px': 0.8180143237113953, '5px': 0.8947396278381348}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.141498565673828\n",
      "metrics:  {'epe': 1.8242985010147095, '1px': 0.7065234780311584, '3px': 0.8029232025146484, '5px': 0.843248724937439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  20.527271270751953\n",
      "metrics:  {'epe': 6.954058647155762, '1px': 0.32174479961395264, '3px': 0.5535677075386047, '5px': 0.5973176956176758}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  11.898701667785645\n",
      "metrics:  {'epe': 3.2630157470703125, '1px': 0.5717252492904663, '3px': 0.7806315422058105, '5px': 0.8014127612113953}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.9657206535339355\n",
      "metrics:  {'epe': 1.74257230758667, '1px': 0.4567057490348816, '3px': 0.8435091376304626, '5px': 0.9099153876304626}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  13.412753105163574\n",
      "metrics:  {'epe': 3.9230759143829346, '1px': 0.2812630236148834, '3px': 0.6839908957481384, '5px': 0.7982943058013916}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  9.666059494018555\n",
      "metrics:  {'epe': 2.5817911624908447, '1px': 0.20016276836395264, '3px': 0.6750781536102295, '5px': 0.8795638084411621}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  9.89498519897461\n",
      "metrics:  {'epe': 2.9076006412506104, '1px': 0.4685481786727905, '3px': 0.7629883289337158, '5px': 0.8345898389816284}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.83420729637146\n",
      "metrics:  {'epe': 1.2221076488494873, '1px': 0.5466601848602295, '3px': 0.9453060030937195, '5px': 0.9820768237113953}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.8634538650512695\n",
      "metrics:  {'epe': 0.6150884032249451, '1px': 0.8208659291267395, '3px': 0.9931640625, '5px': 0.999010443687439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.534334182739258\n",
      "metrics:  {'epe': 1.2133959531784058, '1px': 0.5272786617279053, '3px': 0.9397461414337158, '5px': 0.991894543170929}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.5853679180145264\n",
      "metrics:  {'epe': 1.1882942914962769, '1px': 0.5998437404632568, '3px': 0.9343034029006958, '5px': 0.9791015982627869}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  13.419611930847168\n",
      "metrics:  {'epe': 3.897322654724121, '1px': 0.6437956094741821, '3px': 0.759850263595581, '5px': 0.7916536927223206}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  16.72768783569336\n",
      "metrics:  {'epe': 4.625865459442139, '1px': 0.48255860805511475, '3px': 0.7337760329246521, '5px': 0.7499805092811584}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.403956890106201\n",
      "metrics:  {'epe': 1.0455265045166016, '1px': 0.6067708730697632, '3px': 0.9440234899520874, '5px': 0.9994336366653442}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  10.431082725524902\n",
      "metrics:  {'epe': 2.7146530151367188, '1px': 0.51778644323349, '3px': 0.7900976538658142, '5px': 0.849524736404419}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.526120185852051\n",
      "metrics:  {'epe': 1.3624179363250732, '1px': 0.568164050579071, '3px': 0.8346093893051147, '5px': 0.9681575894355774}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.335082054138184\n",
      "metrics:  {'epe': 1.97846257686615, '1px': 0.43603515625, '3px': 0.7623112201690674, '5px': 0.8963737487792969}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.4370763301849365\n",
      "metrics:  {'epe': 1.2095224857330322, '1px': 0.5374804735183716, '3px': 0.95298832654953, '5px': 0.99462890625}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.139464855194092\n",
      "metrics:  {'epe': 1.4435369968414307, '1px': 0.4856705963611603, '3px': 0.8814453482627869, '5px': 0.9636002779006958}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  16.187946319580078\n",
      "metrics:  {'epe': 4.715173244476318, '1px': 0.33205080032348633, '3px': 0.6464127898216248, '5px': 0.7818424701690674}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  11.115996360778809\n",
      "metrics:  {'epe': 3.160612106323242, '1px': 0.31877604126930237, '3px': 0.77392578125, '5px': 0.8384310007095337}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  12.630400657653809\n",
      "metrics:  {'epe': 4.23446798324585, '1px': 0.5785091519355774, '3px': 0.7516406178474426, '5px': 0.8275976777076721}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.142406940460205\n",
      "metrics:  {'epe': 1.3501977920532227, '1px': 0.4988476634025574, '3px': 0.9087499976158142, '5px': 0.9697591662406921}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  20.241809844970703\n",
      "metrics:  {'epe': 6.607101917266846, '1px': 0.31149739027023315, '3px': 0.6336132884025574, '5px': 0.6828385591506958}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.878619432449341\n",
      "metrics:  {'epe': 1.2828145027160645, '1px': 0.39781901240348816, '3px': 0.9363411664962769, '5px': 0.9827864766120911}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  13.304534912109375\n",
      "metrics:  {'epe': 4.062302112579346, '1px': 0.38805991411209106, '3px': 0.6805533766746521, '5px': 0.7600781321525574}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.101329803466797\n",
      "metrics:  {'epe': 1.2960071563720703, '1px': 0.6346223950386047, '3px': 0.8746809959411621, '5px': 0.9528385400772095}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.819037914276123\n",
      "metrics:  {'epe': 1.2297711372375488, '1px': 0.6467187404632568, '3px': 0.896263062953949, '5px': 0.9587565064430237}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  16.384931564331055\n",
      "metrics:  {'epe': 5.324676036834717, '1px': 0.2729036509990692, '3px': 0.5030273795127869, '5px': 0.6124023795127869}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.633673667907715\n",
      "metrics:  {'epe': 1.9121570587158203, '1px': 0.543860673904419, '3px': 0.7892838716506958, '5px': 0.8977994918823242}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.07607364654541\n",
      "metrics:  {'epe': 1.4312349557876587, '1px': 0.6211068034172058, '3px': 0.8315950632095337, '5px': 0.9175260663032532}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.578075408935547\n",
      "metrics:  {'epe': 1.3205699920654297, '1px': 0.5184114575386047, '3px': 0.9198828339576721, '5px': 0.9607552289962769}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.371428966522217\n",
      "metrics:  {'epe': 0.78300541639328, '1px': 0.8064648509025574, '3px': 0.9021744728088379, '5px': 0.9598893523216248}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.552889823913574\n",
      "metrics:  {'epe': 2.004300832748413, '1px': 0.37524089217185974, '3px': 0.8292903900146484, '5px': 0.8953906297683716}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.268535614013672\n",
      "metrics:  {'epe': 1.3180001974105835, '1px': 0.5734961032867432, '3px': 0.8827148675918579, '5px': 0.9763932824134827}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.041905403137207\n",
      "metrics:  {'epe': 1.5712878704071045, '1px': 0.4617578089237213, '3px': 0.8190039396286011, '5px': 0.980455756187439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.771742820739746\n",
      "metrics:  {'epe': 1.4386026859283447, '1px': 0.5352343916893005, '3px': 0.854687511920929, '5px': 0.9554622769355774}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.273921966552734\n",
      "metrics:  {'epe': 2.061736822128296, '1px': 0.4076562523841858, '3px': 0.7878320217132568, '5px': 0.9131771326065063}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.022728681564331\n",
      "metrics:  {'epe': 0.9698205590248108, '1px': 0.747057318687439, '3px': 0.9098307490348816, '5px': 0.961901068687439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  16.918689727783203\n",
      "metrics:  {'epe': 6.625391483306885, '1px': 0.3000130355358124, '3px': 0.5985677242279053, '5px': 0.6785677075386047}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  9.34688663482666\n",
      "metrics:  {'epe': 3.103142261505127, '1px': 0.1318880170583725, '3px': 0.6547916531562805, '5px': 0.7862955927848816}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.215299129486084\n",
      "metrics:  {'epe': 1.4835213422775269, '1px': 0.3985546827316284, '3px': 0.8923372626304626, '5px': 0.9941536784172058}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.957566738128662\n",
      "metrics:  {'epe': 2.1797358989715576, '1px': 0.3267578184604645, '3px': 0.82380211353302, '5px': 0.9157096743583679}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.153259754180908\n",
      "metrics:  {'epe': 1.0507984161376953, '1px': 0.6074870228767395, '3px': 0.9541992545127869, '5px': 0.9955403804779053}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.3929243087768555\n",
      "metrics:  {'epe': 2.1951992511749268, '1px': 0.30411458015441895, '3px': 0.7634700536727905, '5px': 0.9059374928474426}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.6548194885253906\n",
      "metrics:  {'epe': 0.815567672252655, '1px': 0.7332357168197632, '3px': 0.9902018308639526, '5px': 0.99937504529953}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.31304407119751\n",
      "metrics:  {'epe': 2.1997058391571045, '1px': 0.25418621301651, '3px': 0.8009830713272095, '5px': 0.8905078172683716}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.4564127922058105\n",
      "metrics:  {'epe': 0.6789897084236145, '1px': 0.8084375262260437, '3px': 0.9572916626930237, '5px': 0.9906706213951111}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  13.239856719970703\n",
      "metrics:  {'epe': 4.119106769561768, '1px': 0.24477213621139526, '3px': 0.6460612416267395, '5px': 0.7653450965881348}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.14739727973938\n",
      "metrics:  {'epe': 0.9871247410774231, '1px': 0.75544273853302, '3px': 0.93833988904953, '5px': 0.9590039253234863}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.472720623016357\n",
      "metrics:  {'epe': 1.8335380554199219, '1px': 0.3632942736148834, '3px': 0.8205468654632568, '5px': 0.9460546970367432}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.540269374847412\n",
      "metrics:  {'epe': 0.8437905311584473, '1px': 0.6987825632095337, '3px': 0.9465755224227905, '5px': 0.9922786951065063}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.4562838077545166\n",
      "metrics:  {'epe': 1.1299235820770264, '1px': 0.54959636926651, '3px': 0.92613285779953, '5px': 0.9734309911727905}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.349393367767334\n",
      "metrics:  {'epe': 2.023414373397827, '1px': 0.43453776836395264, '3px': 0.8107031583786011, '5px': 0.9158529043197632}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.6745355129241943\n",
      "metrics:  {'epe': 0.7199432253837585, '1px': 0.7449870109558105, '3px': 0.9776172041893005, '5px': 0.9956445693969727}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.101050853729248\n",
      "metrics:  {'epe': 1.802769422531128, '1px': 0.4395703375339508, '3px': 0.7685286402702332, '5px': 0.9322786927223206}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.0573291778564453\n",
      "metrics:  {'epe': 0.6483650803565979, '1px': 0.7713671922683716, '3px': 0.9665820598602295, '5px': 0.9913476705551147}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.0192482471466064\n",
      "metrics:  {'epe': 0.6813579797744751, '1px': 0.7933268547058105, '3px': 0.9528190493583679, '5px': 0.9988216161727905}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  0.5987970232963562\n",
      "metrics:  {'epe': 0.16264331340789795, '1px': 0.9687044620513916, '3px': 0.9964258074760437, '5px': 0.9996744990348816}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.020964622497559\n",
      "metrics:  {'epe': 1.884857177734375, '1px': 0.5064127445220947, '3px': 0.716601550579071, '5px': 0.9330599308013916}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  26.22698402404785\n",
      "metrics:  {'epe': 8.117646217346191, '1px': 0.4536718726158142, '3px': 0.7290690541267395, '5px': 0.7455729246139526}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.3504433631896973\n",
      "metrics:  {'epe': 1.1545236110687256, '1px': 0.5171809792518616, '3px': 0.9590885639190674, '5px': 0.9981575608253479}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.754333972930908\n",
      "metrics:  {'epe': 2.332343816757202, '1px': 0.4047200679779053, '3px': 0.7575260400772095, '5px': 0.8709049820899963}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  39.82244110107422\n",
      "metrics:  {'epe': 12.342365264892578, '1px': 0.55517578125, '3px': 0.7181640863418579, '5px': 0.72412109375}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  51.63488006591797\n",
      "metrics:  {'epe': 15.268613815307617, '1px': 0.35886719822883606, '3px': 0.5004622340202332, '5px': 0.5241601467132568}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  30.33484649658203\n",
      "metrics:  {'epe': 8.64284896850586, '1px': 0.4898502826690674, '3px': 0.7319596409797668, '5px': 0.7493294477462769}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  23.922300338745117\n",
      "metrics:  {'epe': 5.977938652038574, '1px': 0.34925130009651184, '3px': 0.6391015648841858, '5px': 0.7483398914337158}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.69112229347229\n",
      "metrics:  {'epe': 0.8139281272888184, '1px': 0.7146679759025574, '3px': 0.9407747387886047, '5px': 0.9871484637260437}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.449580669403076\n",
      "metrics:  {'epe': 1.3926644325256348, '1px': 0.528515636920929, '3px': 0.8895443081855774, '5px': 0.9672461152076721}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.498405933380127\n",
      "metrics:  {'epe': 0.8098780512809753, '1px': 0.7384896278381348, '3px': 0.9568619728088379, '5px': 0.9994856715202332}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.8255276679992676\n",
      "metrics:  {'epe': 0.48788782954216003, '1px': 0.8832942843437195, '3px': 0.9850521087646484, '5px': 0.9933463931083679}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  18.587614059448242\n",
      "metrics:  {'epe': 7.296041011810303, '1px': 0.32652994990348816, '3px': 0.4961067736148834, '5px': 0.5659765601158142}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  11.365079879760742\n",
      "metrics:  {'epe': 2.9482972621917725, '1px': 0.3017122447490692, '3px': 0.67369145154953, '5px': 0.7906575798988342}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.0624799728393555\n",
      "metrics:  {'epe': 0.9893396496772766, '1px': 0.7046223878860474, '3px': 0.8713216185569763, '5px': 0.986803412437439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.5714945793151855\n",
      "metrics:  {'epe': 2.3312571048736572, '1px': 0.38141927123069763, '3px': 0.7433333396911621, '5px': 0.8969922065734863}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.1793012619018555\n",
      "metrics:  {'epe': 1.564158320426941, '1px': 0.5141992568969727, '3px': 0.8273047208786011, '5px': 0.9728776216506958}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.865171909332275\n",
      "metrics:  {'epe': 1.4787510633468628, '1px': 0.543860673904419, '3px': 0.8526042103767395, '5px': 0.9390755295753479}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.4781529903411865\n",
      "metrics:  {'epe': 1.021131157875061, '1px': 0.5841536521911621, '3px': 0.970299482345581, '5px': 0.999987006187439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.0967907905578613\n",
      "metrics:  {'epe': 0.7106457948684692, '1px': 0.7616601586341858, '3px': 0.9971094131469727, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.492741107940674\n",
      "metrics:  {'epe': 1.1431571245193481, '1px': 0.5131445527076721, '3px': 0.9596028923988342, '5px': 0.9949153661727905}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.751076698303223\n",
      "metrics:  {'epe': 1.2140991687774658, '1px': 0.5937174558639526, '3px': 0.8989909291267395, '5px': 0.9829687476158142}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.7331323623657227\n",
      "metrics:  {'epe': 0.9290497303009033, '1px': 0.6894010305404663, '3px': 0.9457812905311584, '5px': 0.9863020777702332}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.0690813064575195\n",
      "metrics:  {'epe': 1.1221585273742676, '1px': 0.5224609375, '3px': 0.9843555092811584, '5px': 0.9932942986488342}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.520960569381714\n",
      "metrics:  {'epe': 0.7705195546150208, '1px': 0.7925456166267395, '3px': 0.9643164277076721, '5px': 0.9830794334411621}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.784733772277832\n",
      "metrics:  {'epe': 1.265614628791809, '1px': 0.5533854365348816, '3px': 0.9149804711341858, '5px': 0.9874349236488342}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.6713218688964844\n",
      "metrics:  {'epe': 0.8522554039955139, '1px': 0.7026497721672058, '3px': 0.9448047280311584, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  8.673223495483398\n",
      "metrics:  {'epe': 2.691056966781616, '1px': 0.2958528697490692, '3px': 0.7516992092132568, '5px': 0.8666276335716248}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.694133758544922\n",
      "metrics:  {'epe': 1.073380708694458, '1px': 0.619615912437439, '3px': 0.9475520849227905, '5px': 0.9999609589576721}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.938796043395996\n",
      "metrics:  {'epe': 1.0834325551986694, '1px': 0.5219075679779053, '3px': 0.9716341495513916, '5px': 0.9996680021286011}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.0937697887420654\n",
      "metrics:  {'epe': 0.6442309617996216, '1px': 0.8015820384025574, '3px': 0.9750390648841858, '5px': 0.9959179759025574}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.5193196535110474\n",
      "metrics:  {'epe': 0.4385511875152588, '1px': 0.8907682299613953, '3px': 0.9986523389816284, '5px': 0.9996159076690674}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.8413102626800537\n",
      "metrics:  {'epe': 1.1437175273895264, '1px': 0.7421159148216248, '3px': 0.8438281416893005, '5px': 0.9546484351158142}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.9047534465789795\n",
      "metrics:  {'epe': 1.2433432340621948, '1px': 0.6670442819595337, '3px': 0.7730664014816284, '5px': 0.9873502850532532}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.926211833953857\n",
      "metrics:  {'epe': 1.5781131982803345, '1px': 0.69929039478302, '3px': 0.8601497411727905, '5px': 0.883079469203949}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.744288682937622\n",
      "metrics:  {'epe': 1.2123395204544067, '1px': 0.5831640958786011, '3px': 0.9135026335716248, '5px': 0.9671354293823242}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.1939656734466553\n",
      "metrics:  {'epe': 0.6683051586151123, '1px': 0.7629817724227905, '3px': 0.9925065636634827, '5px': 0.9948828220367432}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  9.800103187561035\n",
      "metrics:  {'epe': 2.4527034759521484, '1px': 0.18599610030651093, '3px': 0.7796158790588379, '5px': 0.9113932251930237}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.278728008270264\n",
      "metrics:  {'epe': 1.2138904333114624, '1px': 0.5406184792518616, '3px': 0.9065625071525574, '5px': 0.9731640815734863}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.024095058441162\n",
      "metrics:  {'epe': 1.9680227041244507, '1px': 0.4130729138851166, '3px': 0.7724804878234863, '5px': 0.9314973950386047}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.370239734649658\n",
      "metrics:  {'epe': 1.4337295293807983, '1px': 0.4797656238079071, '3px': 0.8738737106323242, '5px': 0.9986523389816284}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.099923133850098\n",
      "metrics:  {'epe': 1.5733917951583862, '1px': 0.4045247435569763, '3px': 0.8579167127609253, '5px': 0.984947919845581}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.711099147796631\n",
      "metrics:  {'epe': 1.3734097480773926, '1px': 0.48705729842185974, '3px': 0.9234700798988342, '5px': 0.9872656464576721}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.418118953704834\n",
      "metrics:  {'epe': 2.096756935119629, '1px': 0.39087241888046265, '3px': 0.7725260853767395, '5px': 0.8639127612113953}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.261237621307373\n",
      "metrics:  {'epe': 0.728761613368988, '1px': 0.7556706070899963, '3px': 0.98074871301651, '5px': 0.9989128112792969}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.50629186630249\n",
      "metrics:  {'epe': 1.2656878232955933, '1px': 0.5107421875, '3px': 0.916588544845581, '5px': 0.9899870157241821}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.232048511505127\n",
      "metrics:  {'epe': 2.08707857131958, '1px': 0.3392448127269745, '3px': 0.7714062929153442, '5px': 0.9431966543197632}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.8742847442626953\n",
      "metrics:  {'epe': 1.1975136995315552, '1px': 0.4445573091506958, '3px': 0.9588606953620911, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  23.476280212402344\n",
      "metrics:  {'epe': 7.904847145080566, '1px': 0.38104817271232605, '3px': 0.7430013418197632, '5px': 0.8156380653381348}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.6624113321304321\n",
      "metrics:  {'epe': 0.5292086005210876, '1px': 0.8631119728088379, '3px': 0.9724153876304626, '5px': 0.9916601777076721}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.741800308227539\n",
      "metrics:  {'epe': 0.7606026530265808, '1px': 0.7872721552848816, '3px': 0.9376367330551147, '5px': 0.9888346791267395}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.9552786350250244\n",
      "metrics:  {'epe': 0.9229087829589844, '1px': 0.5890299677848816, '3px': 0.9722135663032532, '5px': 0.9885026216506958}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.8899593353271484\n",
      "metrics:  {'epe': 1.1529322862625122, '1px': 0.6496614813804626, '3px': 0.9042578339576721, '5px': 0.9658203125}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  0.7942525744438171\n",
      "metrics:  {'epe': 0.21420128643512726, '1px': 0.9956966638565063, '3px': 0.9999479651451111, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.1103413105010986\n",
      "metrics:  {'epe': 0.9846578240394592, '1px': 0.6868619918823242, '3px': 0.9636133313179016, '5px': 0.9839974045753479}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  0.8882176280021667\n",
      "metrics:  {'epe': 0.21455033123493195, '1px': 0.9962565302848816, '3px': 0.9997721314430237, '5px': 0.9998698234558105}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.5281131267547607\n",
      "metrics:  {'epe': 0.5008199214935303, '1px': 0.9279688000679016, '3px': 0.9910937547683716, '5px': 0.9917122721672058}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.8460923433303833\n",
      "metrics:  {'epe': 0.5778316259384155, '1px': 0.8361458778381348, '3px': 0.9989908933639526, '5px': 0.9997721314430237}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.6753783226013184\n",
      "metrics:  {'epe': 0.5229752063751221, '1px': 0.8122526407241821, '3px': 0.9996549487113953, '5px': 0.999987006187439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.6440539360046387\n",
      "metrics:  {'epe': 0.8238576650619507, '1px': 0.735130250453949, '3px': 0.98863285779953, '5px': 0.99442058801651}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.869073390960693\n",
      "metrics:  {'epe': 1.611118197441101, '1px': 0.6484895944595337, '3px': 0.8333333730697632, '5px': 0.9112499952316284}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.1929984092712402\n",
      "metrics:  {'epe': 0.3308909833431244, '1px': 0.880566418170929, '3px': 0.9999349117279053, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.3356499671936035\n",
      "metrics:  {'epe': 1.4085227251052856, '1px': 0.48077476024627686, '3px': 0.9111263155937195, '5px': 0.9744661450386047}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.099581241607666\n",
      "metrics:  {'epe': 2.226440191268921, '1px': 0.5287695527076721, '3px': 0.7555078268051147, '5px': 0.8313736915588379}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.338923692703247\n",
      "metrics:  {'epe': 0.9697530269622803, '1px': 0.6509049534797668, '3px': 0.9209961295127869, '5px': 0.9674674868583679}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.5646865367889404\n",
      "metrics:  {'epe': 0.5291354656219482, '1px': 0.86509770154953, '3px': 0.9816601872444153, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.671298027038574\n",
      "metrics:  {'epe': 1.3535717725753784, '1px': 0.4594270884990692, '3px': 0.9220963716506958, '5px': 0.9964518547058105}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.699920415878296\n",
      "metrics:  {'epe': 0.8449203968048096, '1px': 0.6872656345367432, '3px': 0.976061224937439, '5px': 0.9978646039962769}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.436413764953613\n",
      "metrics:  {'epe': 2.2454192638397217, '1px': 0.6857617497444153, '3px': 0.8195247650146484, '5px': 0.8744205832481384}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.305192947387695\n",
      "metrics:  {'epe': 1.6998921632766724, '1px': 0.6617448329925537, '3px': 0.8403776288032532, '5px': 0.904296875}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.5465996265411377\n",
      "metrics:  {'epe': 1.1398738622665405, '1px': 0.761985719203949, '3px': 0.8824479579925537, '5px': 0.9378971457481384}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.707134246826172\n",
      "metrics:  {'epe': 0.8536343574523926, '1px': 0.6605143547058105, '3px': 0.9495182633399963, '5px': 0.9997395873069763}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.367990016937256\n",
      "metrics:  {'epe': 1.2894469499588013, '1px': 0.6040169596672058, '3px': 0.896751344203949, '5px': 0.9709635376930237}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.9935414791107178\n",
      "metrics:  {'epe': 0.971180260181427, '1px': 0.6810482144355774, '3px': 0.93903648853302, '5px': 0.9977799654006958}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.073659420013428\n",
      "metrics:  {'epe': 1.6109613180160522, '1px': 0.52657550573349, '3px': 0.8861132860183716, '5px': 0.9459831118583679}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  24.4222469329834\n",
      "metrics:  {'epe': 7.052725791931152, '1px': 0.5711393356323242, '3px': 0.72565758228302, '5px': 0.7430729269981384}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.013364315032959\n",
      "metrics:  {'epe': 0.958221435546875, '1px': 0.68338543176651, '3px': 0.9394075870513916, '5px': 0.9787369966506958}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.538034439086914\n",
      "metrics:  {'epe': 1.1392322778701782, '1px': 0.6110937595367432, '3px': 0.8833463788032532, '5px': 0.9958593845367432}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  9.024279594421387\n",
      "metrics:  {'epe': 3.070725202560425, '1px': 0.41105470061302185, '3px': 0.7160026431083679, '5px': 0.8556510806083679}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.2825586795806885\n",
      "metrics:  {'epe': 1.0054622888565063, '1px': 0.639355480670929, '3px': 0.9616861939430237, '5px': 0.998522162437439}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.347978115081787\n",
      "metrics:  {'epe': 0.6847766637802124, '1px': 0.7893880605697632, '3px': 0.9751432538032532, '5px': 0.9989843964576721}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.015155792236328\n",
      "metrics:  {'epe': 0.8706201314926147, '1px': 0.6374479532241821, '3px': 0.978430986404419, '5px': 0.9972330927848816}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.1237974166870117\n",
      "metrics:  {'epe': 0.9110718965530396, '1px': 0.6805989742279053, '3px': 0.986523449420929, '5px': 0.9909830689430237}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.9228841066360474\n",
      "metrics:  {'epe': 0.6351251602172852, '1px': 0.7977018356323242, '3px': 0.9689518213272095, '5px': 0.9949153661727905}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.0900590419769287\n",
      "metrics:  {'epe': 0.9456141591072083, '1px': 0.6361783742904663, '3px': 0.9773502945899963, '5px': 0.999218761920929}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.881303548812866\n",
      "metrics:  {'epe': 1.246235966682434, '1px': 0.7060351967811584, '3px': 0.9320638179779053, '5px': 0.9644466638565063}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.535539150238037\n",
      "metrics:  {'epe': 1.4334098100662231, '1px': 0.67389976978302, '3px': 0.8776888251304626, '5px': 0.9208464026451111}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.4161800146102905\n",
      "metrics:  {'epe': 0.3918546140193939, '1px': 0.9111784100532532, '3px': 0.999987006187439, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.900853157043457\n",
      "metrics:  {'epe': 1.2245897054672241, '1px': 0.5861979126930237, '3px': 0.9111849069595337, '5px': 0.9765560030937195}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.743542671203613\n",
      "metrics:  {'epe': 1.2255042791366577, '1px': 0.6600651144981384, '3px': 0.8766536712646484, '5px': 0.9466015696525574}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.1168642044067383\n",
      "metrics:  {'epe': 0.9168316721916199, '1px': 0.6515364646911621, '3px': 0.9887109398841858, '5px': 0.9997005462646484}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.23199725151062\n",
      "metrics:  {'epe': 0.9541223645210266, '1px': 0.7176823019981384, '3px': 0.9757161736488342, '5px': 0.9840559959411621}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.2679877281188965\n",
      "metrics:  {'epe': 0.7049373388290405, '1px': 0.7578645944595337, '3px': 0.9890950918197632, '5px': 0.9982161521911621}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.0337793827056885\n",
      "metrics:  {'epe': 0.5499154329299927, '1px': 0.8428059816360474, '3px': 0.9966015815734863, '5px': 0.9997656345367432}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.331719398498535\n",
      "metrics:  {'epe': 0.713131844997406, '1px': 0.7361393570899963, '3px': 0.9667317867279053, '5px': 0.9948828220367432}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.6080881357192993\n",
      "metrics:  {'epe': 0.4637434482574463, '1px': 0.9299739599227905, '3px': 0.993736982345581, '5px': 0.9994661808013916}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.7700161933898926\n",
      "metrics:  {'epe': 1.2467750310897827, '1px': 0.60867840051651, '3px': 0.8509505391120911, '5px': 0.9741016030311584}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.182592868804932\n",
      "metrics:  {'epe': 1.2526911497116089, '1px': 0.660937488079071, '3px': 0.8946940302848816, '5px': 0.9630794525146484}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.017566680908203\n",
      "metrics:  {'epe': 0.9752388596534729, '1px': 0.7307877540588379, '3px': 0.92369145154953, '5px': 0.9902409315109253}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.314198017120361\n",
      "metrics:  {'epe': 2.0193593502044678, '1px': 0.5538606643676758, '3px': 0.74560546875, '5px': 0.8868164420127869}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.870297908782959\n",
      "metrics:  {'epe': 2.234985113143921, '1px': 0.51171875, '3px': 0.71519535779953, '5px': 0.8292578458786011}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.434879779815674\n",
      "metrics:  {'epe': 1.6975041627883911, '1px': 0.5719922184944153, '3px': 0.8060547113418579, '5px': 0.9450455904006958}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.6789350509643555\n",
      "metrics:  {'epe': 1.822886347770691, '1px': 0.5983138084411621, '3px': 0.8901106715202332, '5px': 0.941601574420929}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.2400572299957275\n",
      "metrics:  {'epe': 0.7132039666175842, '1px': 0.75767582654953, '3px': 0.9999805092811584, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.1289730072021484\n",
      "metrics:  {'epe': 0.972388744354248, '1px': 0.6802409291267395, '3px': 0.928626298904419, '5px': 0.9811719059944153}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.579665184020996\n",
      "metrics:  {'epe': 0.8164403438568115, '1px': 0.7069596648216248, '3px': 0.999987006187439, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.085517406463623\n",
      "metrics:  {'epe': 1.3535674810409546, '1px': 0.6008138060569763, '3px': 0.8507487177848816, '5px': 0.95563805103302}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.4575934410095215\n",
      "metrics:  {'epe': 1.1951137781143188, '1px': 0.7157747745513916, '3px': 0.9367383122444153, '5px': 0.9775586128234863}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.692889213562012\n",
      "metrics:  {'epe': 1.5058313608169556, '1px': 0.426445335149765, '3px': 0.8969987034797668, '5px': 0.9529687762260437}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.1912126541137695\n",
      "metrics:  {'epe': 2.219388484954834, '1px': 0.436210960149765, '3px': 0.8027995228767395, '5px': 0.9133008122444153}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.248854398727417\n",
      "metrics:  {'epe': 1.0556981563568115, '1px': 0.72900390625, '3px': 0.8723763227462769, '5px': 0.9966080784797668}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.8329901695251465\n",
      "metrics:  {'epe': 1.603774905204773, '1px': 0.5161328315734863, '3px': 0.871289074420929, '5px': 0.9449805021286011}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.449488401412964\n",
      "metrics:  {'epe': 1.159420132637024, '1px': 0.7524349093437195, '3px': 0.9475716352462769, '5px': 0.9762109518051147}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.380754232406616\n",
      "metrics:  {'epe': 0.779057502746582, '1px': 0.7254752516746521, '3px': 0.9625521302223206, '5px': 0.9990364909172058}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.6428353786468506\n",
      "metrics:  {'epe': 0.5343130826950073, '1px': 0.8394206166267395, '3px': 0.9517968893051147, '5px': 0.9901302456855774}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.5546751022338867\n",
      "metrics:  {'epe': 1.1170127391815186, '1px': 0.6924414038658142, '3px': 0.8755403757095337, '5px': 0.9813020825386047}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  16.849082946777344\n",
      "metrics:  {'epe': 5.26324987411499, '1px': 0.567187488079071, '3px': 0.7374088764190674, '5px': 0.7662109732627869}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.1888394355773926\n",
      "metrics:  {'epe': 0.9707396626472473, '1px': 0.7493880391120911, '3px': 0.8941797018051147, '5px': 0.9702604413032532}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  18.356678009033203\n",
      "metrics:  {'epe': 5.643121242523193, '1px': 0.5169857144355774, '3px': 0.725182294845581, '5px': 0.7531836032867432}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.890955924987793\n",
      "metrics:  {'epe': 1.7647736072540283, '1px': 0.5517122745513916, '3px': 0.8160417079925537, '5px': 0.9154687523841858}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.940184593200684\n",
      "metrics:  {'epe': 1.9040908813476562, '1px': 0.4807812571525574, '3px': 0.83893883228302, '5px': 0.9300130605697632}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.275303602218628\n",
      "metrics:  {'epe': 0.6961902379989624, '1px': 0.7915234565734863, '3px': 0.9995052218437195, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.8045456409454346\n",
      "metrics:  {'epe': 0.5482759475708008, '1px': 0.8874414563179016, '3px': 0.9953385591506958, '5px': 0.9980403780937195}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  5.612461566925049\n",
      "metrics:  {'epe': 2.0499770641326904, '1px': 0.7008398771286011, '3px': 0.8557682633399963, '5px': 0.8897265791893005}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.717735290527344\n",
      "metrics:  {'epe': 1.643656849861145, '1px': 0.5458658933639526, '3px': 0.8740494847297668, '5px': 0.9378646016120911}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.832490921020508\n",
      "metrics:  {'epe': 2.180295705795288, '1px': 0.6470963954925537, '3px': 0.8016927242279053, '5px': 0.8439778685569763}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  17.65739631652832\n",
      "metrics:  {'epe': 5.60708475112915, '1px': 0.4483463764190674, '3px': 0.6737174391746521, '5px': 0.73953777551651}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.64347505569458\n",
      "metrics:  {'epe': 0.45762524008750916, '1px': 0.8628841638565063, '3px': 0.9992057681083679, '5px': 1.0}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.4208924770355225\n",
      "metrics:  {'epe': 0.7547749280929565, '1px': 0.7703645825386047, '3px': 0.9575651288032532, '5px': 0.994055986404419}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.3057221174240112\n",
      "metrics:  {'epe': 0.41659387946128845, '1px': 0.9033854603767395, '3px': 0.9765104651451111, '5px': 0.9983333349227905}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.2587919235229492\n",
      "metrics:  {'epe': 0.39085111021995544, '1px': 0.8873047232627869, '3px': 0.9990234375, '5px': 0.9998567700386047}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.1568365097045898\n",
      "metrics:  {'epe': 0.3473452031612396, '1px': 0.9315885901451111, '3px': 0.98660808801651, '5px': 0.9994075894355774}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  1.017308235168457\n",
      "metrics:  {'epe': 0.29404982924461365, '1px': 0.9500976800918579, '3px': 0.9823893308639526, '5px': 0.9841927289962769}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.62145733833313\n",
      "metrics:  {'epe': 0.8378646373748779, '1px': 0.6957226991653442, '3px': 0.9425716400146484, '5px': 0.9866406321525574}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.955214023590088\n",
      "metrics:  {'epe': 1.4582339525222778, '1px': 0.5268294215202332, '3px': 0.9362435340881348, '5px': 0.9595833420753479}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.035540580749512\n",
      "metrics:  {'epe': 1.2170459032058716, '1px': 0.6385677456855774, '3px': 0.8836588859558105, '5px': 0.9522461295127869}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.213865280151367\n",
      "metrics:  {'epe': 2.0808393955230713, '1px': 0.46805340051651, '3px': 0.7133724093437195, '5px': 0.8741341233253479}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  3.1982715129852295\n",
      "metrics:  {'epe': 1.0458685159683228, '1px': 0.7200390696525574, '3px': 0.9049218893051147, '5px': 0.9558398723602295}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  2.180166482925415\n",
      "metrics:  {'epe': 0.6175202131271362, '1px': 0.801744818687439, '3px': 0.9982161521911621, '5px': 0.9992057681083679}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  8.561301231384277\n",
      "metrics:  {'epe': 2.82778000831604, '1px': 0.28643229603767395, '3px': 0.6562630534172058, '5px': 0.8471224308013916}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.3826823234558105\n",
      "metrics:  {'epe': 1.943610429763794, '1px': 0.5231835842132568, '3px': 0.8421810269355774, '5px': 0.92626953125}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.586237907409668\n",
      "metrics:  {'epe': 1.4781467914581299, '1px': 0.4511067867279053, '3px': 0.89871746301651, '5px': 0.95584636926651}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  4.232744216918945\n",
      "metrics:  {'epe': 1.3496122360229492, '1px': 0.6769661903381348, '3px': 0.847923219203949, '5px': 0.9250456094741821}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  8.469842910766602\n",
      "metrics:  {'epe': 2.7089364528656006, '1px': 0.4433659017086029, '3px': 0.7293164134025574, '5px': 0.8355990052223206}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  7.88232421875\n",
      "metrics:  {'epe': 2.3762354850769043, '1px': 0.27157554030418396, '3px': 0.7575651407241821, '5px': 0.9138216376304626}\n",
      "input_layer0: torch.Size([8, 64, 160, 240])\n",
      "grad0: torch.Size([8, 1, 160, 240])\n",
      "output_layer0: torch.Size([8, 64, 160, 240])\n",
      "input_layer1: torch.Size([8, 64, 80, 120])\n",
      "grad1: torch.Size([8, 1, 80, 120])\n",
      "output_layer1: torch.Size([8, 128, 80, 120])\n",
      "input_layer2: torch.Size([8, 128, 40, 60])\n",
      "grad2: torch.Size([8, 1, 40, 60])\n",
      "output_layer2: torch.Size([8, 256, 40, 60])\n",
      "output of non-uniform encoding: torch.Size([8, 256, 20, 30])\n",
      "--------------------------------------------------------\n",
      "iter: 12\n",
      "0\n",
      "torch.Size([4, 2, 160, 240])\n",
      "1\n",
      "torch.Size([4, 2, 160, 240])\n",
      "2\n",
      "torch.Size([4, 2, 160, 240])\n",
      "3\n",
      "torch.Size([4, 2, 160, 240])\n",
      "4\n",
      "torch.Size([4, 2, 160, 240])\n",
      "5\n",
      "torch.Size([4, 2, 160, 240])\n",
      "6\n",
      "torch.Size([4, 2, 160, 240])\n",
      "7\n",
      "torch.Size([4, 2, 160, 240])\n",
      "8\n",
      "torch.Size([4, 2, 160, 240])\n",
      "9\n",
      "torch.Size([4, 2, 160, 240])\n",
      "10\n",
      "torch.Size([4, 2, 160, 240])\n",
      "11\n",
      "torch.Size([4, 2, 160, 240])\n",
      "loss:  6.826533794403076\n",
      "metrics:  {'epe': 1.7946765422821045, '1px': 0.43692708015441895, '3px': 0.7987630367279053, '5px': 0.9475716352462769}\n"
     ]
    }
   ],
   "source": [
    "add_noise = True\n",
    "use_mix_attn = True\n",
    "for epoch in range(epochs):\n",
    "    for i_batch, data_blob in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        image1, image2, flow, valid = [x.cuda() for x in data_blob]\n",
    "\n",
    "        if add_noise:\n",
    "            stdv = np.random.uniform(0.0, 5.0)\n",
    "            image1 = (image1 + stdv * torch.randn(*image1.shape).cuda()).clamp(0.0, 255.0)\n",
    "            image2 = (image2 + stdv * torch.randn(*image2.shape).cuda()).clamp(0.0, 255.0)\n",
    "\n",
    "        flow_predictions = model(image1, image2, iters=iter)\n",
    "\n",
    "        loss, metrics = sequence_loss(flow_predictions, image1, image2, flow, valid, gamma=gamma, use_matching_loss=use_mix_attn)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        print(\"loss: \", loss.item())\n",
    "        print(\"metrics: \", metrics)\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "    break\n",
    "PATH = 'checkpoints/carla_new_model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gmflownet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea889b4f18389ce537e0a99e2c7adda3aff0d59bdb54979f5ee747b2578237f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
